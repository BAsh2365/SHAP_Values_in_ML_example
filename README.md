Using Shapley's feature plots as well as a dependency plot to show one way to interpret a Machine Learning Model.

Using a dataset about stress/anxiety levels in students, found on Kaggle, the questions we are trying to answer are as follows:

1. How do certain variables affect the stress level of students? 
2. Which feature is the most important?
3. Which feature has the least impact?

References are found in the Jupyter notebook, but I will place them in the README as well:

- https://shap.readthedocs.io/en/latest/example_notebooks/overviews/An%20introduction%20to%20explainable%20AI%20with%20Shapley%20values.html#Explaining-a-non-additive-boosted-tree-model

- Inspiration/learning: https://www.kaggle.com/code/prashant111/explain-your-model-predictions-with-shapley-values/notebook

- https://christophm.github.io/interpretable-ml-book/shapley.html#shapley

- https://mlu-explain.github.io/random-forest/

- https://www.datacamp.com/tutorial/random-forests-classifier-python

- https://pianalytix.com/random-forest-classifier-and-regressor/

- https://www.datacamp.com/tutorial/introduction-to-shap-values-machine-learning-interpretability

- https://github.com/shap/shap

- Dataset: https://www.kaggle.com/datasets/rxnach/student-stress-factors-a-comprehensive-analysis/data
